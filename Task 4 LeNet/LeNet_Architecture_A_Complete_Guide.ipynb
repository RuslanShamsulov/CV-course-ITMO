{
  "cells": [
    {
      "metadata": {
        "id": "Pkpcuds8gWJv"
      },
      "cell_type": "markdown",
      "source": [
        "## LeNet-5 Complete Architecture\n",
        "\n",
        "LeNet-5, from the paper Gradient-Based Learning Applied to Document Recognition, is a very efficient convolutional neural network for handwritten character recognition.\n",
        "\n",
        "\n",
        "<a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\" target=\"_blank\">Paper: <u>Gradient-Based Learning Applied to Document Recognition</u></a>\n",
        "\n",
        "**Authors**: Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner<br>\n",
        "**Published in**: Proceedings of the IEEE (1998)\n",
        "\n",
        "## Structure of the LeNet network\n",
        "\n",
        "LeNet5 is a small network, it contains the basic modules of deep learning: convolutional layer, pooling layer, and full link layer. It is the basis of other deep learning models. Here we analyze LeNet5 in depth. At the same time, through example analysis, deepen the understanding of the convolutional layer and pooling layer.\n",
        "\n",
        "![lenet](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/lenet-5.png)\n",
        "\n",
        "\n",
        "LeNet-5 Total seven layer , does not comprise an input, each containing a trainable parameters; each layer has a plurality of the Map the Feature , a characteristic of each of the input FeatureMap extracted by means of a convolution filter, and then each FeatureMap There are multiple neurons.\n",
        "\n",
        "![lenet1](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/arch.jpg)\n",
        "\n",
        "Detailed explanation of each layer parameter:\n",
        "\n",
        "### **INPUT LAYER**\n",
        "\n",
        "The first is the data INPUT layer. The size of the input image is uniformly normalized to 32 * 32.\n",
        "\n",
        "> Note: This layer does not count as the network structure of LeNet-5. Traditionally, the input layer is not considered as one of the network hierarchy.\n",
        "\n",
        "\n",
        "### **C1 layer-convolutional layer:**\n",
        "- **Input picture**: 32 * 32\n",
        "- **Convolution kernel size**: 5 * 5\n",
        "- **Convolution kernel types**: 6\n",
        "- **Output featuremap size**: 28 * 28 (32-5 + 1) = 28\n",
        "- **Number of neurons**: 28 * 28 * 6\n",
        "- **Trainable parameters**: (5 * 5 + 1) * 6 (5 * 5 = 25 unit parameters and one bias parameter per filter, a total of 6 filters)\n",
        "- **Number of connections**: (5 * 5 + 1) * 6 * 28 * 28 = 122304\n",
        "\n",
        "#### **Detailed description:**\n",
        "- The first convolution operation is performed on the input image (using 6 convolution kernels of size 5 * 5) to obtain 6 C1 feature maps (6 feature maps of size 28 * 28, 32-5 + 1 = 28).\n",
        "\n",
        "- Let's take a look at how many parameters are needed. The size of the convolution kernel is 5 * 5, and there are 6 * (5 * 5 + 1) = 156 parameters in total, where +1 indicates that a kernel has a bias.\n",
        "\n",
        "- For the convolutional layer C1, each pixel in C1 is connected to 5 * 5 pixels and 1 bias in the input image, so there are 156 * 28 * 28 = 122304 connections in total. There are 122,304 connections, but we only need to learn 156 parameters, mainly through weight sharing.\n",
        "\n",
        "\n",
        "### **S2 layer-pooling layer (downsampling layer):**\n",
        "- **Input**: 28 * 28\n",
        "- **Sampling area**: 2 * 2\n",
        "- **Sampling method**: 4 inputs are added, multiplied by a trainable parameter, plus a trainable offset. Results via sigmoid\n",
        "- **Sampling type**: 6\n",
        "- **Output featureMap size**: 14 * 14 (28/2)\n",
        "- **Number of neurons**: 14 * 14 * 6\n",
        "- **Trainable parameters**: 2 * 6 (the weight of the sum + the offset)\n",
        "- **Number of connections**: (2 * 2 + 1) * 6 * 14 * 14\n",
        "- The size of each feature map in S2 is 1/4 of the size of the feature map in C1.\n",
        "\n",
        "#### **Detailed description:**\n",
        "- The pooling operation is followed immediately after the first convolution. Pooling is performed using 2 * 2 kernels, and S2, 6 feature maps of 14 * 14 (28/2 = 14) are obtained.\n",
        "\n",
        "- The pooling layer of S2 is the sum of the pixels in the 2 * 2 area in C1 multiplied by a weight coefficient plus an offset, and then the result is mapped again.\n",
        "\n",
        "- So each pooling core has two training parameters, so there are 2x6 = 12 training parameters, but there are 5x14x14x6 = 5880 connections.\n",
        "\n",
        "\n",
        "### **C3 layer-convolutional layer:**\n",
        "\n",
        "- **Input**: all 6 or several feature map combinations in S2\n",
        "- **Convolution kernel size**: 5 * 5\n",
        "- **Convolution kernel type**: 16\n",
        "- **Output featureMap size**: 10 * 10 (14-5 + 1) = 10\n",
        "- Each feature map in C3 is connected to all 6 or several feature maps in S2, indicating that the feature map of this layer is a different combination of the feature maps extracted from the previous layer.\n",
        "- One way is that the first 6 feature maps of C3 take 3 adjacent feature map subsets in S2 as input. The next 6 feature maps take 4 subsets of neighboring feature maps in S2 as input. The next three take the non-adjacent 4 feature map subsets as input. The last one takes all the feature maps in S2 as input.\n",
        "- **The trainable parameters are**: 6 * (3 * 5 * 5 + 1) + 6 * (4 * 5 * 5 + 1) + 3 * (4 * 5 * 5 + 1) + 1 * (6 * 5 * 5 +1) = 1516\n",
        "- **Number of connections**: 10 * 10 * 1516 = 151600\n",
        "\n",
        "#### **Detailed description:**\n",
        "\n",
        "- After the first pooling, the second convolution, the output of the second convolution is C3, 16 10x10 feature maps, and the size of the convolution kernel is 5 * 5. We know that S2 has 6 14 * 14 feature maps, how to get 16 feature maps from 6 feature maps? Here are the 16 feature maps calculated by the special combination of the feature maps of S2. details as follows:\n",
        "\n",
        "- The first 6 feature maps of C3 (corresponding to the 6th column of the first red box in the figure above) are connected to the 3 feature maps connected to the S2 layer (the first red box in the above figure), and the next 6 feature maps are connected to the S2 layer The 4 feature maps are connected (the second red box in the figure above), the next 3 feature maps are connected with the 4 feature maps that are not connected at the S2 layer, and the last is connected with all the feature maps at the S2 layer. The convolution kernel size is still 5 * 5, so there are 6 * (3 * 5 * 5 + 1) + 6 * (4 * 5 * 5 + 1) + 3 * (4 * 5 * 5 + 1) +1 * (6 * 5 * 5 + 1) = 1516 parameters. The image size is 10 * 10, so there are 151600 connections.\n",
        "\n",
        "![lenet1](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/c31.png)\n",
        "\n",
        "- The convolution structure of C3 and the first 3 graphs in S2 is shown below:\n",
        "\n",
        "![lenet1](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/c32.png)\n",
        "\n",
        "\n",
        "### **S4 layer-pooling layer (downsampling layer)**\n",
        "- **Input**: 10 * 10\n",
        "- **Sampling area**: 2 * 2\n",
        "- **Sampling method**: 4 inputs are added, multiplied by a trainable parameter, plus a trainable offset. Results via sigmoid\n",
        "- **Sampling type**: 16\n",
        "- **Output featureMap size**: 5 * 5 (10/2)\n",
        "- **Number of neurons**: 5 * 5 * 16 = 400\n",
        "- **Trainable parameters**: 2 * 16 = 32 (the weight of the sum + the offset)\n",
        "- **Number of connections**: 16 * (2 * 2 + 1) * 5 * 5 = 2000\n",
        "- The size of each feature map in S4 is 1/4 of the size of the feature map in C3\n",
        "\n",
        "#### **Detailed description:**\n",
        "- S4 is the pooling layer, the window size is still 2 * 2, a total of 16 feature maps, and the 16 10x10 maps of the C3 layer are pooled in units of 2x2 to obtain 16 5x5 feature maps. This layer has a total of 32 training parameters of 2x16, 5x5x5x16 = 2000 connections.\n",
        "\n",
        "> *The connection is similar to the S2 layer.*\n",
        "\n",
        "### **C5 layer-convolution layer**\n",
        "- **Input**: All 16 unit feature maps of the S4 layer (all connected to s4)\n",
        "- **Convolution kernel size**: 5 * 5\n",
        "- **Convolution kernel type**: 120\n",
        "- **Output featureMap size**: 1 * 1 (5-5 + 1)\n",
        "- **Trainable parameters / connection**: 120 * (16 * 5 * 5 + 1) = 48120\n",
        "\n",
        "#### **Detailed description:**\n",
        "- The C5 layer is a convolutional layer. Since the size of the 16 images of the S4 layer is 5x5, which is the same as the size of the convolution kernel, the size of the image formed after convolution is 1x1. This results in 120 convolution results. Each is connected to the 16 maps on the previous level. So there are (5x5x16 + 1) x120 = 48120 parameters, and there are also 48120 connections. The network structure of the C5 layer is as follows:\n",
        "\n",
        "![lenet1](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/c5.png)\n",
        "\n",
        "\n",
        "### **F6 layer-fully connected layer**\n",
        "- **Input**: c5 120-dimensional vector\n",
        "- **Calculation method**: calculate the dot product between the input vector and the weight vector, plus an offset, and the result is output through the sigmoid function.\n",
        "- **Trainable parameters**: 84 * (120 + 1) = 10164\n",
        "\n",
        "#### **Detailed description:**\n",
        "- Layer 6 is a fully connected layer. The F6 layer has 84 nodes, corresponding to a 7x12 bitmap, -1 means white, 1 means black, so the black and white of the bitmap of each symbol corresponds to a code. The training parameters and number of connections for this layer are (120 + 1) x84 = 10164. The ASCII encoding diagram is as follows:\n",
        "\n",
        "![lenet1](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/f61.png)\n",
        "\n",
        "- The connection method of the F6 layer is as follows:\n",
        "\n",
        "![lenet1](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/f62.png)\n",
        "\n",
        "\n",
        "### **Output layer-fully connected layer**\n",
        "\n",
        "- The output layer is also a fully connected layer, with a total of 10 nodes, which respectively represent the numbers 0 to 9, and if the value of node i is 0, the result of network recognition is the number i. A radial basis function (RBF) network connection is used. Assuming x is the input of the previous layer and y is the output of the RBF, the calculation of the RBF output is:\n",
        "\n",
        "![lenet1](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/81.png)\n",
        "\n",
        "- The value of the above formula w_ij is determined by the bitmap encoding of i, where i ranges from 0 to 9, and j ranges from 0 to 7 * 12-1. The closer the value of the RBF output is to 0, the closer it is to i, that is, the closer to the ASCII encoding figure of i, it means that the recognition result input by the current network is the character i. This layer has 84x10 = 840 parameters and connections.\n",
        "\n",
        "![lenet1](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/82.png)\n",
        "\n",
        "\n",
        "## **Summary**\n",
        "* LeNet-5 is a very efficient convolutional neural network for handwritten character recognition.\n",
        "* Convolutional neural networks can make good use of the structural information of images.\n",
        "* The convolutional layer has fewer parameters, which is also determined by the main characteristics of the convolutional layer, that is, local connection and shared weights."
      ]
    },
    {
      "metadata": {
        "id": "u9fxFD65gWJ0"
      },
      "cell_type": "markdown",
      "source": [
        "### Code Implementation"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Y8RK9dFYgWJ1"
      },
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SIqPY7V-gWJ2"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading Dataset"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3ZedftqYgWJ2",
        "outputId": "12a94542-df0c-4d18-da13-f06502cc992b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Loading the dataset and perform splitting\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CVsBvGgWgWJ2"
      },
      "cell_type": "markdown",
      "source": [
        "### Image Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lRZP9S3igWJ3"
      },
      "cell_type": "code",
      "source": [
        "# Peforming reshaping operation\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Normalization\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "# One Hot Encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8X2uPIO4gWJ3"
      },
      "cell_type": "markdown",
      "source": [
        "### LeNet Model Architecture"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VgOCQRLHgWJ3",
        "outputId": "24b8043a-9854-47bd-f22a-2d8b4348dd3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Building the Model Architecture\n",
        "model = Sequential()\n",
        "# Select 6 feature convolution kernels with a size of 5 * 5 (without offset), and get 66 feature maps. The size of each feature map is 32â5 + 1 = 2832â5 + 1 = 28.\n",
        "# That is, the number of neurons has been reduced from 10241024 to 28 â 28 = 784 28 â 28 = 784.\n",
        "# Parameters between input layer and C1 layer: 6 â (5 â 5 + 1)\n",
        "model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
        "# The input of this layer is the output of the first layer, which is a 28 * 28 * 6 node matrix.\n",
        "# The size of the filter used in this layer is 2 * 2, and the step length and width are both 2, so the output matrix size of this layer is 14 * 14 * 6.\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# The input matrix size of this layer is 14 * 14 * 6, the filter size used is 5 * 5, and the depth is 16. This layer does not use all 0 padding, and the step size is 1.\n",
        "# The output matrix size of this layer is 10 * 10 * 16. This layer has 5 * 5 * 6 * 16 + 16 = 2416 parameters\n",
        "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
        "# The input matrix size of this layer is 10 * 10 * 16. The size of the filter used in this layer is 2 * 2, and the length and width steps are both 2, so the output matrix size of this layer is 5 * 5 * 16.\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# The input matrix size of this layer is 5 * 5 * 16. This layer is called a convolution layer in the LeNet-5 paper, but because the size of the filter is 5 * 5, #\n",
        "# So it is not different from the fully connected layer. If the nodes in the 5 * 5 * 16 matrix are pulled into a vector, then this layer is the same as the fully connected layer.\n",
        "# The number of output nodes in this layer is 120, with a total of 5 * 5 * 16 * 120 + 120 = 48120 parameters.\n",
        "model.add(Flatten())\n",
        "model.add(Dense(120, activation='relu'))\n",
        "# The number of input nodes in this layer is 120 and the number of output nodes is 84. The total parameter is 120 * 84 + 84 = 10164 (w + b)\n",
        "model.add(Dense(84, activation='relu'))\n",
        "# The number of input nodes in this layer is 84 and the number of output nodes is 10. The total parameter is 84 * 10 + 10 = 850\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "qOziTC04gWJ4"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "wmmf049wgWJ4",
        "outputId": "b0154326-b623-4cc9-dd03-700f8e7529c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 55ms/step - accuracy: 0.7904 - loss: 0.7018 - val_accuracy: 0.9731 - val_loss: 0.0872\n",
            "Epoch 2/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 55ms/step - accuracy: 0.9711 - loss: 0.0909 - val_accuracy: 0.9792 - val_loss: 0.0621\n",
            "Epoch 3/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 54ms/step - accuracy: 0.9798 - loss: 0.0660 - val_accuracy: 0.9845 - val_loss: 0.0486\n",
            "Epoch 4/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9850 - loss: 0.0499 - val_accuracy: 0.9865 - val_loss: 0.0412\n",
            "Epoch 5/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 57ms/step - accuracy: 0.9880 - loss: 0.0399 - val_accuracy: 0.9877 - val_loss: 0.0396\n",
            "Epoch 6/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 53ms/step - accuracy: 0.9893 - loss: 0.0333 - val_accuracy: 0.9856 - val_loss: 0.0443\n",
            "Epoch 7/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - accuracy: 0.9909 - loss: 0.0285 - val_accuracy: 0.9894 - val_loss: 0.0371\n",
            "Epoch 8/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9921 - loss: 0.0241 - val_accuracy: 0.9879 - val_loss: 0.0359\n",
            "Epoch 9/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 57ms/step - accuracy: 0.9931 - loss: 0.0218 - val_accuracy: 0.9834 - val_loss: 0.0484\n",
            "Epoch 10/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 53ms/step - accuracy: 0.9935 - loss: 0.0213 - val_accuracy: 0.9891 - val_loss: 0.0342\n",
            "Epoch 11/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - accuracy: 0.9950 - loss: 0.0163 - val_accuracy: 0.9905 - val_loss: 0.0308\n",
            "Epoch 12/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9957 - loss: 0.0133 - val_accuracy: 0.9911 - val_loss: 0.0329\n",
            "Epoch 13/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 64ms/step - accuracy: 0.9958 - loss: 0.0122 - val_accuracy: 0.9894 - val_loss: 0.0349\n",
            "Epoch 14/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9959 - loss: 0.0130 - val_accuracy: 0.9876 - val_loss: 0.0398\n",
            "Epoch 15/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 56ms/step - accuracy: 0.9962 - loss: 0.0116 - val_accuracy: 0.9902 - val_loss: 0.0354\n",
            "Epoch 16/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9964 - loss: 0.0101 - val_accuracy: 0.9876 - val_loss: 0.0432\n",
            "Epoch 17/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 61ms/step - accuracy: 0.9973 - loss: 0.0080 - val_accuracy: 0.9870 - val_loss: 0.0452\n",
            "Epoch 18/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 57ms/step - accuracy: 0.9968 - loss: 0.0092 - val_accuracy: 0.9912 - val_loss: 0.0376\n",
            "Epoch 19/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 54ms/step - accuracy: 0.9977 - loss: 0.0070 - val_accuracy: 0.9887 - val_loss: 0.0479\n",
            "Epoch 20/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - accuracy: 0.9973 - loss: 0.0072 - val_accuracy: 0.9904 - val_loss: 0.0417\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d7e46398150>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6B8wMJxggWJ4",
        "outputId": "337c0e0b-1b03-4997-82c9-2161619330aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test)\n",
        "print('Test Loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9877 - loss: 0.0519\n",
            "Test Loss: 0.04171505570411682\n",
            "Test accuracy: 0.9904000163078308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÐÐ°Ð¼ÐµÐ½Ð° Ð½Ð° Average Polling"
      ],
      "metadata": {
        "id": "FwzEKmGokPc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import AveragePooling2D\n",
        "# Building the Model Architecture\n",
        "mode_new = Sequential()\n",
        "# Select 6 feature convolution kernels with a size of 5 * 5 (without offset), and get 66 feature maps. The size of each feature map is 32â5 + 1 = 2832â5 + 1 = 28.\n",
        "# That is, the number of neurons has been reduced from 10241024 to 28 â 28 = 784 28 â 28 = 784.\n",
        "# Parameters between input layer and C1 layer: 6 â (5 â 5 + 1)\n",
        "mode_new.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
        "# The input of this layer is the output of the first layer, which is a 28 * 28 * 6 node matrix.\n",
        "# The size of the filter used in this layer is 2 * 2, and the step length and width are both 2, so the output matrix size of this layer is 14 * 14 * 6.\n",
        "mode_new.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "# The input matrix size of this layer is 14 * 14 * 6, the filter size used is 5 * 5, and the depth is 16. This layer does not use all 0 padding, and the step size is 1.\n",
        "# The output matrix size of this layer is 10 * 10 * 16. This layer has 5 * 5 * 6 * 16 + 16 = 2416 parameters\n",
        "mode_new.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
        "# The input matrix size of this layer is 10 * 10 * 16. The size of the filter used in this layer is 2 * 2, and the length and width steps are both 2, so the output matrix size of this layer is 5 * 5 * 16.\n",
        "mode_new.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "# The input matrix size of this layer is 5 * 5 * 16. This layer is called a convolution layer in the LeNet-5 paper, but because the size of the filter is 5 * 5, #\n",
        "# So it is not different from the fully connected layer. If the nodes in the 5 * 5 * 16 matrix are pulled into a vector, then this layer is the same as the fully connected layer.\n",
        "# The number of output nodes in this layer is 120, with a total of 5 * 5 * 16 * 120 + 120 = 48120 parameters.\n",
        "mode_new.add(Flatten())\n",
        "mode_new.add(Dense(120, activation='relu'))\n",
        "# The number of input nodes in this layer is 120 and the number of output nodes is 84. The total parameter is 120 * 84 + 84 = 10164 (w + b)\n",
        "mode_new.add(Dense(84, activation='relu'))\n",
        "# The number of input nodes in this layer is 84 and the number of output nodes is 10. The total parameter is 84 * 10 + 10 = 850\n",
        "mode_new.add(Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "WJ5DEZ5NkRWm",
        "outputId": "aad2c22f-3995-48af-f542-eb4f543ec446",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mode_new.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "KWabIxPUmQWu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode_new.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "ICBQvkC3mZR2",
        "outputId": "a0a52c56-08bd-4ae8-d1c2-6dd672a98335",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 53ms/step - accuracy: 0.7582 - loss: 0.7942 - val_accuracy: 0.9529 - val_loss: 0.1636\n",
            "Epoch 2/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 52ms/step - accuracy: 0.9535 - loss: 0.1528 - val_accuracy: 0.9736 - val_loss: 0.0850\n",
            "Epoch 3/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 50ms/step - accuracy: 0.9718 - loss: 0.0920 - val_accuracy: 0.9770 - val_loss: 0.0755\n",
            "Epoch 4/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 56ms/step - accuracy: 0.9786 - loss: 0.0706 - val_accuracy: 0.9842 - val_loss: 0.0504\n",
            "Epoch 5/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 52ms/step - accuracy: 0.9822 - loss: 0.0580 - val_accuracy: 0.9833 - val_loss: 0.0530\n",
            "Epoch 6/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.9846 - loss: 0.0488 - val_accuracy: 0.9841 - val_loss: 0.0497\n",
            "Epoch 7/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.9876 - loss: 0.0390 - val_accuracy: 0.9879 - val_loss: 0.0392\n",
            "Epoch 8/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 56ms/step - accuracy: 0.9883 - loss: 0.0366 - val_accuracy: 0.9873 - val_loss: 0.0378\n",
            "Epoch 9/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 53ms/step - accuracy: 0.9894 - loss: 0.0319 - val_accuracy: 0.9882 - val_loss: 0.0382\n",
            "Epoch 10/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.9906 - loss: 0.0289 - val_accuracy: 0.9880 - val_loss: 0.0403\n",
            "Epoch 11/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 51ms/step - accuracy: 0.9925 - loss: 0.0229 - val_accuracy: 0.9892 - val_loss: 0.0358\n",
            "Epoch 12/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 52ms/step - accuracy: 0.9927 - loss: 0.0225 - val_accuracy: 0.9880 - val_loss: 0.0378\n",
            "Epoch 13/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.9941 - loss: 0.0180 - val_accuracy: 0.9872 - val_loss: 0.0408\n",
            "Epoch 14/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.9944 - loss: 0.0172 - val_accuracy: 0.9869 - val_loss: 0.0445\n",
            "Epoch 15/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 51ms/step - accuracy: 0.9940 - loss: 0.0169 - val_accuracy: 0.9893 - val_loss: 0.0380\n",
            "Epoch 16/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - accuracy: 0.9952 - loss: 0.0148 - val_accuracy: 0.9884 - val_loss: 0.0405\n",
            "Epoch 17/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.9958 - loss: 0.0121 - val_accuracy: 0.9903 - val_loss: 0.0344\n",
            "Epoch 18/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 52ms/step - accuracy: 0.9965 - loss: 0.0103 - val_accuracy: 0.9905 - val_loss: 0.0372\n",
            "Epoch 19/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.9968 - loss: 0.0107 - val_accuracy: 0.9905 - val_loss: 0.0380\n",
            "Epoch 20/20\n",
            "\u001b[1m469/469\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 56ms/step - accuracy: 0.9953 - loss: 0.0127 - val_accuracy: 0.9901 - val_loss: 0.0331\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ed9765dd110>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = mode_new.evaluate(x_test, y_test)\n",
        "print('Test Loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "qRrYk6XntFG4",
        "outputId": "48e8a0ce-c026-4bc4-812d-d81a653efdeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9872 - loss: 0.0430\n",
            "Test Loss: 0.03308440372347832\n",
            "Test accuracy: 0.9901000261306763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ÐÐ°ÑÐ°ÑÐµÑ Ñ Ð±ÑÐºÐ²Ð°Ð¼Ð¸**"
      ],
      "metadata": {
        "id": "wPMq_ilptxFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoaderWrapEMNIST:\n",
        "    def __init__(self, batch = 64):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        self.train_data = datasets.EMNIST(\n",
        "            root=\"./data\",\n",
        "            split = 'letters',\n",
        "            train=True,\n",
        "            transform=transform,\n",
        "            download=True\n",
        "        )\n",
        "\n",
        "        self.test_data = datasets.EMNIST(\n",
        "            root=\"./data\",\n",
        "            split = 'letters',\n",
        "            train=False,\n",
        "            transform=transform,\n",
        "            download=True\n",
        "        )\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=batch,\n",
        "            shuffle=True\n",
        "            )\n",
        "\n",
        "        self.test_loader = DataLoader(\n",
        "            self.test_data,\n",
        "            batch_size=batch,\n",
        "            shuffle=False\n",
        "            )\n",
        "\n",
        "        self.class_names = self.train_data.classes"
      ],
      "metadata": {
        "id": "02c3jqKrt0Ts"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# ÐÐ»Ð°ÑÑ Ð´Ð»Ñ Ð·Ð°Ð³ÑÑÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½ÑÑ EMNIST\n",
        "class DataLoaderWrapEMNIST:\n",
        "    def __init__(self, batch=64):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "        self.train_data = datasets.EMNIST(\n",
        "            root=\"./data\",\n",
        "            split='letters',\n",
        "            train=True,\n",
        "            transform=transform,\n",
        "            download=True\n",
        "        )\n",
        "\n",
        "        self.test_data = datasets.EMNIST(\n",
        "            root=\"./data\",\n",
        "            split='letters',\n",
        "            train=False,\n",
        "            transform=transform,\n",
        "            download=True\n",
        "        )\n",
        "\n",
        "        # ÐÑÐ¿ÑÐ°Ð²Ð»ÑÐµÐ¼ Ð¼ÐµÑÐºÐ¸, Ð²ÑÑÐ¸ÑÐ°Ñ 1\n",
        "        self.train_data.targets = self.train_data.targets - 1\n",
        "        self.test_data.targets = self.test_data.targets - 1\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=batch,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        self.test_loader = DataLoader(\n",
        "            self.test_data,\n",
        "            batch_size=batch,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        self.class_names = self.train_data.classes\n",
        "\n",
        "# ÐÐ¾Ð´ÐµÐ»Ñ LeNet\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.pool1 = nn.AvgPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.pool2 = nn.AvgPool2d(2)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 26)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)  # flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# ÐÐ½Ð¸ÑÐ¸Ð°Ð»Ð¸Ð·Ð°ÑÐ¸Ñ Ð´Ð°Ð½Ð½ÑÑ Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "data_loader = DataLoaderWrapEMNIST(batch=64)\n",
        "train_loader = data_loader.train_loader\n",
        "test_loader = data_loader.test_loader\n",
        "\n",
        "model = LeNet()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# ÐÐ°ÑÑÑÐ¾Ð¹ÐºÐ° Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¾ÑÐ° Ð¸ ÑÑÐ½ÐºÑÐ¸Ð¸ Ð¿Ð¾ÑÐµÑÑ\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ÐÐ±ÑÑÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "def train_model(model, train_loader, optimizer, criterion, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100*correct/total:.2f}%\")\n",
        "\n",
        "# ÐÑÐµÐ½ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Test Accuracy: {100*correct/total:.2f}%\")\n",
        "\n",
        "# ÐÐ±ÑÑÐµÐ½Ð¸Ðµ Ð¸ Ð¾ÑÐµÐ½ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "train_model(model, train_loader, optimizer, criterion, num_epochs=5)\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "g6tBtfnauXnt",
        "outputId": "e8d86081-42f2-4aa2-95db-aed481fc4398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.7000, Accuracy: 78.42%\n",
            "Epoch 2/5, Loss: 0.3054, Accuracy: 90.01%\n",
            "Epoch 3/5, Loss: 0.2518, Accuracy: 91.53%\n",
            "Epoch 4/5, Loss: 0.2233, Accuracy: 92.45%\n",
            "Epoch 5/5, Loss: 0.2044, Accuracy: 92.98%\n",
            "Test Accuracy: 92.38%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "LeNet Architecture: A Complete Guide",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}